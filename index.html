<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Recasting to TNLI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
      <section class="page-header">
        <h1>Recasting for Table-NLI</h1>
        <a href="https://arxiv.org/abs/2211.12641" class="btn">Paper</a>
		<a href="https://drive.google.com/drive/folders/18TRXD6_Ba4CHKtqY7w9cQK33IcPao8M6?usp=sharing" class="btn">Datasets</a>
        <a href="https://github.com/recasting-to-nli/Recasting-to-TNLI-Source" class="btn">Code</a>
      <!--  <a href="explore.html" class="btn">Explore</a> -->
        <a href="https://docs.google.com/presentation/d/1kf_ykqQ7iyESZLow0FOruvZ4TkNSc85eeg7h8-noT9A/edit?usp=sharing" class="btn">Slides</a><br>
      </section>

    <section class="main-content">
      <h1>Leveraging Data Recasting to Enhance Tabular Reasoning</h1>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2><p style="text-align: justify;">Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic generation, which is scalable and cost effective but lacks inventiveness. In this research, we present a framework for semi-automatically recasting existing tabular data to make use of the benefits of both approaches. We utilize our framework to build tabular NLI instances from five datasets that were initially intended for tasks like table2text creation, tabular Q/A, and semantic parsing. We demonstrate that recasted data could be used as evaluation benchmarks as well as augmentation data to enhance performance on tabular NLI tasks. Furthermore, we investigate the effectiveness of models trained on recasted data in the zero-shot scenario, and analyse trends in performance across different recasted datasets types. <p>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Framework</h2>
			<figure>
				<img src="figures/example.png" style="max-width:100%;">
				<figcaption> 
					Pipeline for generating recasted NLI data. We first create entailments and contradictions from the given
					base annotation. We then create a counterfactual table taking a contradiction to be the new base annotation.
					subscriptOG represents the “Original” table and subscriptCF represents the “Counterfactual” table. Note that Base
					EntailmentOG contradicts TableCF and Base EntailmentCF contradicts TableOG. This pair will always exhibit this
					property, but there can be statements which entail (or contradict) both OG and CF tables.
				</figcaption>
			</figure>

			<h2>Source Datasets and Recasted Datasets</h2>
			<p style="text-align: justify;">
				Using the framework outlined above, we recast the five datasets listed below. All datasets
				utilise open-domain Wikipedia tables, comparable
				to TabFact. In addition, these datasets and TabFact
				share reasoning kinds such as counting, minimum/-
				maximum, ranking, superlatives, comparatives, and
				uniqueness, among others. 
				Source datasets and statistics for various recasted datasets are gieven below. 
				QA-TNLI combines recasted data from both FeTaQA and
				WikiTableQuestions. Test splits are created by randomly sampling 10% samples from each dataset.
			</p>
			<table>
					<thead>
						<tr>
							<th colspan="2">Source Dataset</th>
							<th colspan="3">Generated Dataset</th>
					
						</tr>
						<tr>
							<th>Dataset</th>
							<th>Task</th>
							<th>NLI-Dataset</th>
							<th>Entail | Contradict | Total</th>
						</tr>
					</thead>
					<tbody align="center">
						<tr>
							<td>WikiTableQuestions (Pasupat and Liang, 2015a)</td>
							<td>Table Question Answering</td>
							<td rowspan="2">QA-TNLI</td>
							<td rowspan="2">32k | 77k | 109k</td>
						</tr>
						<tr>
							<td>FeTaQA (Nan et al., 2022)</td>
							<td>Table Question Answering</td>
						</tr>
						<tr>
							<td>WikiSQL (Zhong et al., 2017)</td>
							<td>Table Semantic Parsing</td>
							<td>WikiSQL-TNLI</td>
							<td>300k | 385k | 685k</td>
						</tr>
						<tr>
							<td>Squall (Shi et al., 2020b)</td>
							<td>Table Semantic Parsing</td>
							<td>Squall-TNLI</td>
							<td>105k | 93k | 198k</td>
						</tr>
						<tr>
							<td>ToTTo (Parikh et al., 2020)</td>
							<td>Table To Text Generation</td>
							<td>ToTTo-TNLI</td>
							<td>493k | 357k | 850k</td>
						</tr>
	
					</tbody>
				</table>
			</div>
			<br><br>
			<h2>Experiments and Analysis</h2>
			We examine the relevance of our
			recast data across various settings. Overall, we aim
			to answer the following research questions:
			<ol>
				<li>RQ1: How challenging is recast data as a
					TNLI benchmark?</li>
				<li>RQ2: How effective are models trained on
					recasted data in a zero shot setting?</li>
				<li>RQ3: How beneficial is recasted data for
					TNLI data augmentation?</li>
			</ol> 
		
			<h3>Setup</h3>
			In all experiments, we follow the pre-training
			pipeline similar to Eisenschlos et al. (2020). We
			start with TAPAS (Herzig et al., 2020), a tablebased BERT model, and intermediately pre-train it
			on our recasted data. We then fine-tune the model
			on the downstream tabular NLI task.
			Dataset. We use TabFact (Chen et al., 2020b), a
			benchmark Table NLI dataset, as the end task to report results. TabFact is a binary classification task
			(with labels: Entail, Refute) on Wikipedia derived
			tables. We use the standard train and test splits in
			our experiments, and report the official accuracy
			metric. TabFact gives simple and complex tags to
			each example in its test set, referring to statements
			derived from single and multiple rows respectively.
			Complex statements encompass a range of aggregation functions applied over multiple rows of table
			data. We report and analyze our results on simple
			and complex test data separately

			<h3>Results</h3>

			<figure>
				<img src="figures/results.png" style="max-width:100%;">
				<figcaption> 
					Accuracies on TabFact, including the Human Performance. Table-BERT-Horizontal and LPA-Ranking (w/ discriminator) are baselines taken from TabFact (Chen et al., 2020b). CF means CounterFactual data, TF means TansFormers, LPA means
					Latent Program Algorithm. ToTTo-TNLI, QA-TNLI (WikiTQ + FeTaQA), WikiSQL - TNLI and Squall - TNLI are table NLI
					models pre-trained on CF + Synthetic data (Eisenschlos et al., 2020) followed by respective re-casted datasets. Combined - TNLI
					is a model trained on all of the data, starting with CF + Synthetic data and then mixing data from recasted datasets in equal rates.
				</figcaption>
			</figure>
			
      <footer class="site-footer">
        <span class="site-footer-owner">This website is maintained by <a href="https://github.com/aashnajena">Aashna Jena</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>

    </section>

  </body>
</html>
